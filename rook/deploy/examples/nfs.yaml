#################################################################################################################
# Create a Ceph pool with settings for replication in production environments. A minimum of 3 OSDs on
# different hosts are required in this example.
#  kubectl create -f nfs.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephNFS
metadata:
  name: my-nfs
  namespace: rook-ceph # namespace:cluster
spec:
  # For Ceph v15, the rados block is required. It is ignored for Ceph v16.
  rados:
    # RADOS pool where NFS configs are stored.
    # In this example the data pool for the "myfs" filesystem is used.
    # If using the object store example, the data pool would be "my-store.rgw.buckets.data".
    # Note that this has nothing to do with where exported file systems or object stores live.
    pool: myfs-replicated
    # RADOS namespace where NFS client recovery data is stored in the pool.
    namespace: nfs-ns

  # Settings for the NFS server
  server:
    # the number of active NFS servers
    active: 3
    # where to run the NFS server
    placement:
    #  nodeAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      nodeSelectorTerms:
    #      - matchExpressions:
    #        - key: role
    #          operator: In
    #          values:
    #          - mds-node
    #  topologySpreadConstraints:
    #  tolerations:
    #  - key: mds-node
    #    operator: Exists
    #  podAffinity:
    #  podAntiAffinity:
    # A key/value list of annotations
    annotations:
    #  key: value
    # The requests and limits set here allow the ganesha pod(s) to use half of one CPU core and 1 gigabyte of memory
    resources:
    #  limits:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #  requests:
    #    cpu: "500m"
    #    memory: "1024Mi"
    # the priority class to set to influence the scheduler's pod preemption
    #priorityClassName:
    # The logging levels: NIV_NULL | NIV_FATAL | NIV_MAJ | NIV_CRIT | NIV_WARN | NIV_EVENT | NIV_INFO | NIV_DEBUG | NIV_MID_DEBUG |NIV_FULL_DEBUG |NB_LOG_LEVEL
    logLevel: NIV_INFO
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: builtin-nfs
  namespace: rook-ceph # namespace:cluster
spec:
  # The required pool name ".nfs" cannot be specified as a K8s resource name, thus we override
  # the pool name created in Ceph with this name property
  name: .nfs
  failureDomain: host
  replicated:
    size: 3
    requireSafeReplicaSize: true
